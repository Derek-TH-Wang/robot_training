ginger: #namespace
  task_and_robot_environment_name: "GingerPathPlanning-v0"

  # DQN algorithm parameters
  alpha: 0.002 # Learning Rate
  alpha_decay: 0.01
  gamma: 0.5 # 1-1/(n), n:min step to goal. future action and reward influence to current, the larger gamma, the robot more focus on the future, less gamma, maybe easy to reach target
  epsilon: 0.8 # choose random rate, 0 none 1 a lot, in a complex task, large is beter
  epsilon_decay: 0.999 # how we reduse the exploration, 0.9995: 10000 times will reduce to 0.006
  epsilon_min: 0.01 # minimum value that epsilon can have
  replay_buffer_size: 20000
  batch_size: 10000 # need larger than n_actions!!! as large as possible(but not all data?), otherwise, will loss some of the experience
  episodes_training: 500
  episodes_running: 100
  n_win_ticks: 50 # If the mean of rewards is bigger than this and have passed min_episodes, the task is considered finished
  min_episodes: 10
  monitor: True
  quiet: False

  # Ginger Task Env Realated parameters
  n_actions: 2180 # 3^7
  n_observations: 29 # init angles:7 + goal angles:7 + current angles:7 + every joint distance:7 + joint space distance:1 = 29 (cartesion space distance)
  n_dof: 7
  action_step: 0.1
  # because of the large action space, reward need to be setted as some big number, otherwise, the loss of the neural network will always be tiny
  step_punishment: -100
  step_bonus: 100
  closer_reward_type: 1 # -(reached_goal_reward/(InitDist)^3)*(currentDist - InitDist)^3
  impossible_movement_punishement: -0
  reached_goal_reward: 100

  start_angle:
    joint0: 0.0
    joint1: 0.0
    joint2: 0.0
    joint3: 0.0
    joint4: 0.0
    joint5: 0.0
    joint6: 0.0

  goal_angle:
    joint0: -0.2
    joint1: 0.2
    joint2: 0.1
    joint3: -0.2
    joint4: -0.2
    joint5: 0.2
    joint6: 0.0

  max_distance: 1.0 # Maximum distance from EE to the desired GOAL EE

